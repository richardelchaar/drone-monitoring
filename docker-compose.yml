version: '3.8'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    ports:
      - "2181:2181"
    networks:
      kafka-network:
        aliases:
          - zookeeper

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      kafka-network:
        aliases:
          - kafka



  spark:
    image: bitnami/spark:latest
    container_name: spark
    depends_on:
      - kafka
    environment:
      - SPARK_MODE=master
      - SPARK_LOG_LEVEL=WARN
      - SPARK_NO_DAEMONIZE=true
    ports:
      - "7077:7077"  # Spark master port
      - "8082:8082"  # Spark Web UI
    volumes:
      - ./scripts:/app
      - ./checkpoint:/app/checkpoint
    networks:
      kafka-network:
        aliases:
          - spark

  spark-worker:
    image: bitnami/spark:latest
    container_name: spark-worker
    depends_on:
      - spark
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=2
      - SPARK_LOG_LEVEL=WARN
    ports:
      - "8081:8081"  # Spark Worker Web UI
    volumes:
      - ./scripts:/app
      - ./checkpoint:/app/checkpoint
    networks:
      kafka-network:
        aliases:
          - spark-worker

    # Spark Driver (new container to run the Spark job)
  spark-driver:
    image: bitnami/spark:latest
    container_name: spark-driver
    depends_on:
      - spark
    environment:
      - SPARK_MODE=client
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_LOG_LEVEL=WARN
    volumes:
      - ./scripts:/app  # Mount scripts directory
      - ./checkpoint:/app/checkpoint  # Mount checkpoint directory for persistence

    command: >
      bin/spark-submit
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0,com.datastax.spark:spark-cassandra-connector_2.12:3.2.0
      --master spark://spark:7077
      /app/spark_stream.py
    networks:
      kafka-network:
        aliases:
          - spark-driver

  cassandra:
    image: cassandra:latest
    container_name: cassandra
    ports:
      - "9042:9042" # Cassandra's default port
    networks:
      kafka-network:
        aliases:
          - cassandra
    environment:
      - CASSANDRA_CLUSTER_NAME=hydroponics-cluster
      - CASSANDRA_NUM_TOKENS=256
      - CASSANDRA_START_RPC=true
    volumes:
      - cassandra-data:/var/lib/cassandra  # Mount the persistent volume
      - ./scripts/create_tables2.cql:/docker-entrypoint-initdb.d/create_tables2.cql  # Run this script on startup


  data-generator:
    build:
      context: .
    depends_on:
      - kafka
    networks:
      kafka-network:
        aliases:
          - data-generator
    working_dir: /app
    volumes:
      - ./scripts:/app
    command: ["python", "drone_simulator.py"]


  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    environment:
      # Installs a Cassandra datasource plugin upon startup (example plugin)
      - GF_INSTALL_PLUGINS=hadesarchitect-cassandra-datasource
      # Default admin credentials (not recommended in production)
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    ports:
      - "3000:3000"
    networks:
      kafka-network:
        aliases:
          - grafana
    depends_on:
      - cassandra
    volumes:
      # For persisting Grafana dashboards and config
      - grafana-data:/var/lib/grafana





networks:
  kafka-network:
    driver: bridge

volumes:
  cassandra-data:  # Define the volume for persistence
  grafana-data:
    driver: local